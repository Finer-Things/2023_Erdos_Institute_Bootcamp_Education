{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import math \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../Data/Expendatures Teacher Inexperience Out of Cert and Core Index\").dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['log_PER_FED_STATE_LOCAL_EXP'] = np.log(df['PER_FED_STATE_LOCAL_EXP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Unnamed: 0                        ENTITY_NAME     ENTITY_CD  \\\n",
      "0              0           MONTESSORI MAGNET SCHOOL   10100010014   \n",
      "1              1       PINE HILLS ELEMENTARY SCHOOL   10100010016   \n",
      "2              2          DELAWARE COMMUNITY SCHOOL   10100010018   \n",
      "3              3     NEW SCOTLAND ELEMENTARY SCHOOL   10100010019   \n",
      "4              4        ALBANY SCHOOL OF HUMANITIES   10100010023   \n",
      "...          ...                                ...           ...   \n",
      "5734        5734  CLYDE-SAVANNAH JUNIOR HIGH SCHOOL  650301040004   \n",
      "5735        5735            LYONS ELEMENTARY SCHOOL  650501040001   \n",
      "5736        5736           LYONS SENIOR HIGH SCHOOL  650501040002   \n",
      "5737        5737                LYONS MIDDLE SCHOOL  650501040003   \n",
      "5738        5738           MARION ELEMENTARY SCHOOL  650701040001   \n",
      "\n",
      "      PER_FED_STATE_LOCAL_EXP  PER_TEACH_INEXP  PER_OUT_CERT  ABSENT_RATE  \\\n",
      "0                     13960.0              5.0           0.0         12.4   \n",
      "1                     14171.0             10.0           0.0         17.6   \n",
      "2                      9778.0             15.0           0.0         34.1   \n",
      "3                     14548.0             11.0           0.0         17.1   \n",
      "4                     17087.0              9.0           2.0         16.1   \n",
      "...                       ...              ...           ...          ...   \n",
      "5734                  14500.0             16.0           6.0         49.2   \n",
      "5735                  14359.0             11.0           0.0         40.7   \n",
      "5736                  15298.0             30.0           5.0         38.8   \n",
      "5737                  13601.0             29.0           0.0         24.7   \n",
      "5738                  14964.0             33.0           0.0         16.3   \n",
      "\n",
      "      INSTITUTION_ID  CORE_INDEX   SUBJECT SCHOOL_TYPE  \\\n",
      "0       800000055730       165.9  Combined          EM   \n",
      "1       800000055731        89.2  Combined          EM   \n",
      "2       800000055732        61.9  Combined          EM   \n",
      "3       800000055733       125.1  Combined          EM   \n",
      "4       800000055736       120.1  Combined          EM   \n",
      "...              ...         ...       ...         ...   \n",
      "5734    800000070913        81.3  Combined          EM   \n",
      "5735    800000035834        83.1  Combined          EM   \n",
      "5736    800000035835       127.3  Combined          HS   \n",
      "5737    800000035836        76.0  Combined          EM   \n",
      "5738    800000035810       144.0  Combined          EM   \n",
      "\n",
      "      log_PER_FED_STATE_LOCAL_EXP  \n",
      "0                        9.543951  \n",
      "1                        9.558953  \n",
      "2                        9.187890  \n",
      "3                        9.585209  \n",
      "4                        9.746073  \n",
      "...                           ...  \n",
      "5734                     9.581904  \n",
      "5735                     9.572132  \n",
      "5736                     9.635477  \n",
      "5737                     9.517899  \n",
      "5738                     9.613403  \n",
      "\n",
      "[5625 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['PER_TEACH_INEXP', 'log_PER_FED_STATE_LOCAL_EXP', 'PER_OUT_CERT', 'ABSENT_RATE']\n",
    "outcome = \"CORE_INDEX\"\n",
    "#list(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "model_df = df[features + [outcome]].copy()\n",
    "\n",
    "df_train, df_temp = train_test_split(model_df, \n",
    "                                     train_size=0.8, \n",
    "                                     random_state=500, \n",
    "                                     shuffle=True\n",
    "                                    )\n",
    "\n",
    "# Split the remaining data into validation and test sets\n",
    "df_val, df_test = train_test_split(df_temp, \n",
    "                                   train_size=0.5,  # Assuming you want half of the remaining data for validation and half for testing\n",
    "                                   random_state=500, \n",
    "                                   shuffle=True\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train[features]\n",
    "y_train = df_train[outcome]\n",
    "\n",
    "X_val = df_val[features]\n",
    "y_val = df_val[outcome]\n",
    "\n",
    "X_test = df_test[features]\n",
    "y_test = df_test[outcome]\n",
    "\n",
    "# Normalize the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create and fit the linear regression model\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Now you can use the model to make predictions on the validation or test set\n",
    "# For example, on the validation set:\n",
    "predictions_val = linreg.predict(X_val_scaled)\n",
    "\n",
    "# And on the test set:\n",
    "predictions_test = linreg.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Feature  Coefficient\n",
      "0  ABSENT_RATE   -22.576467\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'model' is your trained Linear Regression model\n",
    "coefficients = linreg.coef_\n",
    "\n",
    "# Create a DataFrame to display coefficients along with corresponding feature names\n",
    "coefficients_df = pd.DataFrame({'Feature': features, 'Coefficient': coefficients})\n",
    "\n",
    "# Print or display the coefficients\n",
    "print(coefficients_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Metrics:\n",
      "Mean Absolute Error (MAE): 25.82474528202455\n",
      "Mean Squared Error (MSE): 1019.7356235512435\n",
      "Root Mean Squared Error (RMSE): 31.933299603254962\n",
      "R-squared (R²): 0.33325882239881166\n",
      "\n",
      "Validation Set Metrics:\n",
      "Mean Absolute Error (MAE): 25.431890455808286\n",
      "Mean Squared Error (MSE): 987.4092606052225\n",
      "Root Mean Squared Error (RMSE): 31.423068924044042\n",
      "R-squared (R²): 0.3783931204197115\n",
      "\n",
      "Test Set Metrics:\n",
      "Mean Absolute Error (MAE): 26.528986843365033\n",
      "Mean Squared Error (MSE): 1063.131595938293\n",
      "Root Mean Squared Error (RMSE): 32.60569882609929\n",
      "R-squared (R²): 0.35988014393892565\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Assuming 'model' is your trained Linear Regression model\n",
    "\n",
    "# Make predictions on the training set\n",
    "predictions_train = linreg .predict(X_train_scaled)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "predictions_val = linreg .predict(X_val_scaled)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions_test = linreg .predict(X_test_scaled)\n",
    "\n",
    "# Calculate evaluation metrics for the training set\n",
    "mae_train = mean_absolute_error(y_train, predictions_train)\n",
    "mse_train = mean_squared_error(y_train, predictions_train)\n",
    "rmse_train = np.sqrt(mse_train)\n",
    "r2_train = r2_score(y_train, predictions_train)\n",
    "\n",
    "# Calculate evaluation metrics for the validation set\n",
    "mae_val = mean_absolute_error(y_val, predictions_val)\n",
    "mse_val = mean_squared_error(y_val, predictions_val)\n",
    "rmse_val = np.sqrt(mse_val)\n",
    "r2_val = r2_score(y_val, predictions_val)\n",
    "\n",
    "# Calculate evaluation metrics for the test set\n",
    "mae_test = mean_absolute_error(y_test, predictions_test)\n",
    "mse_test = mean_squared_error(y_test, predictions_test)\n",
    "rmse_test = np.sqrt(mse_test)\n",
    "r2_test = r2_score(y_test, predictions_test)\n",
    "\n",
    "# Print the results for each set\n",
    "print('Training Set Metrics:')\n",
    "print(f'Mean Absolute Error (MAE): {mae_train}')\n",
    "print(f'Mean Squared Error (MSE): {mse_train}')\n",
    "print(f'Root Mean Squared Error (RMSE): {rmse_train}')\n",
    "print(f'R-squared (R²): {r2_train}')\n",
    "print()\n",
    "\n",
    "print('Validation Set Metrics:')\n",
    "print(f'Mean Absolute Error (MAE): {mae_val}')\n",
    "print(f'Mean Squared Error (MSE): {mse_val}')\n",
    "print(f'Root Mean Squared Error (RMSE): {rmse_val}')\n",
    "print(f'R-squared (R²): {r2_val}')\n",
    "print()\n",
    "\n",
    "print('Test Set Metrics:')\n",
    "print(f'Mean Absolute Error (MAE): {mae_test}')\n",
    "print(f'Mean Squared Error (MSE): {mse_test}')\n",
    "print(f'Root Mean Squared Error (RMSE): {rmse_test}')\n",
    "print(f'R-squared (R²): {r2_test}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "def clean_label(label):\n",
    "    return label.replace(\"_\", \" \").title()\n",
    "\n",
    "# Assuming df_test is your test dataset\n",
    "predictions_test = linreg.predict(X_test_scaled)\n",
    "\n",
    "# Create a DataFrame with actual and predicted values for the test set\n",
    "scatter_df_test = pd.DataFrame({\n",
    "    'Actual': y_test,\n",
    "    'Predicted': predictions_test,\n",
    "    'log_PER_FED_STATE_LOCAL_EXP': df_test['log_PER_FED_STATE_LOCAL_EXP'],\n",
    "    'PER_TEACH_INEXP': df_test['PER_TEACH_INEXP'],\n",
    "    'PER_OUT_CERT': df_test['PER_OUT_CERT'],\n",
    "    'CORE_INDEX': df_test['CORE_INDEX'], \n",
    "    'ABSENT_RATE': df_test['ABSENT_RATE']\n",
    "})\n",
    "\n",
    "# Compare predictions on the training set and the test set\n",
    "for feature in features:\n",
    "    figure, axes = plt.subplots(1, 2, figsize=(20, 8), sharey=True)\n",
    "    figure.suptitle(clean_label(feature), fontsize=26)\n",
    "\n",
    "    # Scatter plot for training set actual and predicted data points\n",
    "    sns.scatterplot(data=scatter_df_test, x=feature, y='Actual', label='Training Set - Actual', ax=axes[0])\n",
    "    sns.scatterplot(data=scatter_df_test, x=feature, y='Predicted', label='Training Set - Predicted', ax=axes[0], color=\"darkorange\")\n",
    "\n",
    "    # Scatter plot for test set actual and predicted data points\n",
    "    sns.scatterplot(data=scatter_df_test, x=feature, y='Actual', label='Test Set - Actual', ax=axes[1])\n",
    "    sns.scatterplot(data=scatter_df_test, x=feature, y='Predicted', label='Test Set - Predicted', ax=axes[1], color=\"darkorange\")\n",
    "\n",
    "    axes[0].set_title(\"Training Set - Actual vs Predicted\")\n",
    "    axes[1].set_title(\"Test Set - Actual vs Predicted\")\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So far, the R-squared looks below 0.5, and scatter plots do not present a clear linear relationship. Thus, we try polynomial regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train[features]\n",
    "y_train = df_train[outcome]\n",
    "\n",
    "X_val = df_val[features]\n",
    "y_val = df_val[outcome]\n",
    "\n",
    "X_test = df_test[features]\n",
    "y_test = df_test[outcome]\n",
    "\n",
    "# Normalize the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create a polynomial regression model (degree = 2, you can change it)\n",
    "degree = 4\n",
    "polyreg = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "\n",
    "# Fit the model\n",
    "polyreg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the training set\n",
    "predictions_poly = polyreg.predict(X_train_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Make predictions on the training set\n",
    "predictions_train = polyreg.predict(X_train_scaled)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "predictions_val = polyreg.predict(X_val_scaled)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions_test = polyreg.predict(X_test_scaled)\n",
    "\n",
    "# Calculate evaluation metrics for the training set\n",
    "mae_train = mean_absolute_error(y_train, predictions_train)\n",
    "mse_train = mean_squared_error(y_train, predictions_train)\n",
    "rmse_train = np.sqrt(mse_train)\n",
    "r2_train = r2_score(y_train, predictions_train)\n",
    "\n",
    "# Calculate evaluation metrics for the validation set\n",
    "mae_val = mean_absolute_error(y_val, predictions_val)\n",
    "mse_val = mean_squared_error(y_val, predictions_val)\n",
    "rmse_val = np.sqrt(mse_val)\n",
    "r2_val = r2_score(y_val, predictions_val)\n",
    "\n",
    "# Calculate evaluation metrics for the test set\n",
    "mae_test = mean_absolute_error(y_test, predictions_test)\n",
    "mse_test = mean_squared_error(y_test, predictions_test)\n",
    "rmse_test = np.sqrt(mse_test)\n",
    "r2_test = r2_score(y_test, predictions_test)\n",
    "\n",
    "# Print the results for each set\n",
    "print('Training Set Metrics:')\n",
    "print(f'Mean Absolute Error (MAE): {mae_train}')\n",
    "print(f'Mean Squared Error (MSE): {mse_train}')\n",
    "print(f'Root Mean Squared Error (RMSE): {rmse_train}')\n",
    "print(f'R-squared (R²): {r2_train}')\n",
    "print()\n",
    "\n",
    "print('Validation Set Metrics:')\n",
    "print(f'Mean Absolute Error (MAE): {mae_val}')\n",
    "print(f'Mean Squared Error (MSE): {mse_val}')\n",
    "print(f'Root Mean Squared Error (RMSE): {rmse_val}')\n",
    "print(f'R-squared (R²): {r2_val}')\n",
    "print()\n",
    "\n",
    "print('Test Set Metrics:')\n",
    "print(f'Mean Absolute Error (MAE): {mae_test}')\n",
    "print(f'Mean Squared Error (MSE): {mse_test}')\n",
    "print(f'Root Mean Squared Error (RMSE): {rmse_test}')\n",
    "print(f'R-squared (R²): {r2_test}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "def clean_label(label):\n",
    "    return label.replace(\"_\", \" \").title()\n",
    "\n",
    "# Assuming df_test is your test dataset\n",
    "predictions_test = polyreg.predict(X_test_scaled)\n",
    "\n",
    "# Create a DataFrame with actual and predicted values for the test set\n",
    "scatter_df_test = pd.DataFrame({\n",
    "    'Actual': y_test,\n",
    "    'Predicted': predictions_test,\n",
    "    'log_PER_FED_STATE_LOCAL_EXP': df_test['log_PER_FED_STATE_LOCAL_EXP'],\n",
    "    'PER_TEACH_INEXP': df_test['PER_TEACH_INEXP'],\n",
    "    'PER_OUT_CERT': df_test['PER_OUT_CERT'],\n",
    "    'CORE_INDEX': df_test['CORE_INDEX'], \n",
    "    'ABSENT_RATE': df_test['ABSENT_RATE']\n",
    "})\n",
    "\n",
    "# Compare predictions on the training set and the test set\n",
    "for feature in features:\n",
    "    figure, axes = plt.subplots(1, 2, figsize=(20, 8), sharey=True)\n",
    "    figure.suptitle(clean_label(feature), fontsize=26)\n",
    "\n",
    "    # Scatter plot for training set actual and predicted data points\n",
    "    sns.scatterplot(data=scatter_df_test, x=feature, y='Actual', label='Training Set - Actual', ax=axes[0])\n",
    "    sns.scatterplot(data=scatter_df_test, x=feature, y='Predicted', label='Training Set - Predicted', ax=axes[0], color=\"darkorange\")\n",
    "\n",
    "    # Scatter plot for test set actual and predicted data points\n",
    "    sns.scatterplot(data=scatter_df_test, x=feature, y='Actual', label='Test Set - Actual', ax=axes[1])\n",
    "    sns.scatterplot(data=scatter_df_test, x=feature, y='Predicted', label='Test Set - Predicted', ax=axes[1], color=\"darkorange\")\n",
    "\n",
    "    axes[0].set_title(\"Training Set - Actual vs Predicted\")\n",
    "    axes[1].set_title(\"Test Set - Actual vs Predicted\")\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model Comparison\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "linear_predictions = linreg.predict(X_val_scaled)\n",
    "poly_predictions = polyreg.predict(X_val_scaled)\n",
    "\n",
    "linear_mse = mean_squared_error(y_val, linear_predictions)\n",
    "linear_r2 = r2_score(y_val, linear_predictions)\n",
    "\n",
    "poly_mse = mean_squared_error(y_val, poly_predictions)\n",
    "poly_r2 = r2_score(y_val, poly_predictions)\n",
    "\n",
    "print(\"Linear Model:\")\n",
    "print(f\"MSE: {linear_mse}\")\n",
    "print(f\"R-squared: {linear_r2}\")\n",
    "print()\n",
    "\n",
    "print(\"Polynomial Model:\")\n",
    "print(f\"MSE: {poly_mse}\")\n",
    "print(f\"R-squared: {poly_r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.scatter(y_val, linear_predictions, label='Linear Model', color='blue')\n",
    "plt.scatter(y_val, poly_predictions, label='Polynomial Model', color='orange')\n",
    "\n",
    "plt.plot([min(y_val), max(y_val)], [min(y_val), max(y_val)], linestyle='--', color='gray', label='Ideal Fit')\n",
    "\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('Model Comparison: Linear vs. Polynomial')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Trying something new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_rf = {\n",
    "    'n_estimators': 100,  # adjust as needed\n",
    "    'max_depth': None,  # or specify a value\n",
    "    'min_samples_split': 2,  # adjust as needed\n",
    "    'min_samples_leaf': 1,  # adjust as needed\n",
    "    'max_features': 'auto',  # adjust as needed\n",
    "    'random_state': 42  # set a seed for reproducibility\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestRegressor(**params_rf)\n",
    "rf_model.fit(X_train_scaled, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X_val_scaled is your validation data\n",
    "rf_predictions = rf_model.predict(X_val_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_mse = mean_squared_error(y_val, rf_predictions)\n",
    "rf_r2 = r2_score(y_val, rf_predictions)\n",
    "\n",
    "print(\"Random Forest Model:\")\n",
    "print(f\"MSE: {rf_mse}\")\n",
    "print(f\"R-squared: {rf_r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.scatter(y_val, rf_predictions, label='Random Forest Model', color='purple')\n",
    "\n",
    "plt.plot([min(y_val), max(y_val)], [min(y_val), max(y_val)], linestyle='--', color='gray', label='Ideal Fit')\n",
    "\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('Random Forest Model Prediction')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
